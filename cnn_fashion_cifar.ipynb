{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from cnn.minivggnet import MiniVGGNet\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Fashion MNIST...\n",
      "..done\n"
     ]
    }
   ],
   "source": [
    "# grab the Fashion MNIST dataset (if this is your first time running\n",
    "# this the dataset will be automatically downloaded)\n",
    "print(\"[INFO] loading Fashion MNIST...\")\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "print(\"..done\")\n",
    "\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress and normalize the image by 2X2 plaquettes\n",
    "def compress(image):\n",
    "    lc = np.zeros((int(len(image)/2),len(image)))\n",
    "    for i in range(int(len(image)/2)):\n",
    "        lc[i,:] = image[2*i,:]+image[2*i+1,:]\n",
    "    compress =  np.zeros((int(len(image)/2),int(len(image)/2)))   \n",
    "    for i in range(int(len(image)/2)):\n",
    "        compress[:,i] = lc[:,2*i]+lc[:,2*i+1]\n",
    "    return compress/(np.max(image)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: From  (60000, 28, 28)\n",
      "to compressed  (60000, 14, 14)\n",
      "TEST: From  (10000, 28, 28)\n",
      "to compressed  (10000, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "comp_xtrain = np.zeros((len(trainX),int(trainX.shape[1]/2),int(trainX.shape[2]/2)))\n",
    "for i, image in enumerate(trainX):\n",
    "    comp_xtrain[i] = compress(image)\n",
    "print(\"TRAIN: From \",trainX.shape)\n",
    "print(\"to compressed \",comp_xtrain.shape)\n",
    "\n",
    "comp_xtest = np.zeros((len(testX),int(testX.shape[1]/2),int(testX.shape[2]/2)))\n",
    "for i, image in enumerate(testX):\n",
    "    comp_xtest[i] = compress(image)    \n",
    "print(\"TEST: From \",testX.shape)\n",
    "print(\"to compressed \",comp_xtest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for CNN input (need to specify the depth ~ colour channels)\n",
    "comp_xtrain = comp_xtrain.reshape((trainX.shape[0], 14, 14,1))\n",
    "comp_xtest = comp_xtest.reshape((testX.shape[0], 14, 14,1))\n",
    "# one-hot encode the training and testing labels\n",
    "trainY = np_utils.to_categorical(trainY, 10)\n",
    "testY = np_utils.to_categorical(testY, 10)\n",
    "# initialize the label names\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 185s 3ms/step - loss: 0.7974 - accuracy: 0.7213 - val_loss: 0.5015 - val_accuracy: 0.8196\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 178s 3ms/step - loss: 0.5380 - accuracy: 0.8006 - val_loss: 0.4414 - val_accuracy: 0.8355\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 179s 3ms/step - loss: 0.4957 - accuracy: 0.8164 - val_loss: 0.4232 - val_accuracy: 0.8428\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 0.4703 - accuracy: 0.8248 - val_loss: 0.4050 - val_accuracy: 0.8493\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 180s 3ms/step - loss: 0.4482 - accuracy: 0.8332 - val_loss: 0.3982 - val_accuracy: 0.8496\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 181s 3ms/step - loss: 0.4395 - accuracy: 0.8360 - val_loss: 0.3943 - val_accuracy: 0.8510\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 178s 3ms/step - loss: 0.4309 - accuracy: 0.8408 - val_loss: 0.3827 - val_accuracy: 0.8590\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 180s 3ms/step - loss: 0.4240 - accuracy: 0.8414 - val_loss: 0.3801 - val_accuracy: 0.8576\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 184s 3ms/step - loss: 0.4176 - accuracy: 0.8431 - val_loss: 0.3718 - val_accuracy: 0.8632\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.4132 - accuracy: 0.8461 - val_loss: 0.3761 - val_accuracy: 0.8626\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
    "\n",
    "#model is a CNN, input parameters are the data shape\n",
    "model = MiniVGGNet.build(width= 14, height= 14, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(comp_xtrain, trainY,\n",
    "\tvalidation_data=(comp_xtest, testY),\n",
    "\tbatch_size=BS, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         top       0.82      0.84      0.83      1000\n",
      "     trouser       0.95      0.96      0.96      1000\n",
      "    pullover       0.76      0.82      0.79      1000\n",
      "       dress       0.87      0.86      0.86      1000\n",
      "        coat       0.75      0.76      0.75      1000\n",
      "      sandal       0.98      0.93      0.96      1000\n",
      "       shirt       0.68      0.59      0.63      1000\n",
      "     sneaker       0.88      0.98      0.93      1000\n",
      "         bag       0.97      0.97      0.97      1000\n",
      "  ankle boot       0.97      0.92      0.94      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5416765c50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions on the test set\n",
    "preds = model.predict(comp_xtest)\n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1),\n",
    "\ttarget_names=labelNames))\n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "#plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# X_train is 50000 rows of 3x32x32 values \n",
    "\n",
    "X_train = X_train.reshape(50000,32,32,3)\n",
    "X_test = X_test.reshape(10000,32,32,3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) train samples\n",
      "(50000, 16, 16, 3) compressed\n",
      "(10000, 32, 32, 3) test samples\n",
      "(10000, 16, 16, 3) compressed\n"
     ]
    }
   ],
   "source": [
    "# normalize and compress the datasets\n",
    "comp_xtrain = np.zeros((len(X_train),int(X_train.shape[1]/2),int(X_train.shape[2]/2),3))\n",
    "for i,image in enumerate(X_train):\n",
    "    for color in range(3):\n",
    "        comp_xtrain[i,:,:,color] = compress(image[:,:,color])\n",
    "\n",
    "comp_xtest = np.zeros((len(X_test),int(X_test.shape[1]/2),int(X_test.shape[2]/2),3))\n",
    "for i,image in enumerate(X_test):\n",
    "    for color in range(3):\n",
    "        comp_xtest[i,:,:,color] = compress(image[:,:,color])\n",
    "\n",
    "print(X_train.shape, 'train samples')\n",
    "print(comp_xtrain.shape, 'compressed')\n",
    "print(X_test.shape, 'test samples')\n",
    "print(comp_xtest.shape, 'compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to the range of [0, 1]\n",
    "trainX = comp_xtrain\n",
    "testX = comp_xtest\n",
    "# one-hot encode the training and testing labels\n",
    "trainY = np_utils.to_categorical(y_train, 10)\n",
    "testY = np_utils.to_categorical(y_test, 10)\n",
    "# initialize the label names\n",
    "labelNames = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "              \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 168s 3ms/step - loss: 1.9364 - accuracy: 0.3707 - val_loss: 1.4010 - val_accuracy: 0.4952\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 163s 3ms/step - loss: 1.4441 - accuracy: 0.4857 - val_loss: 1.1918 - val_accuracy: 0.5713\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 1.2963 - accuracy: 0.5346 - val_loss: 1.0787 - val_accuracy: 0.6165\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 131s 3ms/step - loss: 1.2125 - accuracy: 0.5663 - val_loss: 1.0339 - val_accuracy: 0.6315\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 132s 3ms/step - loss: 1.1570 - accuracy: 0.5882 - val_loss: 0.9955 - val_accuracy: 0.6441\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 1.1159 - accuracy: 0.6017 - val_loss: 0.9726 - val_accuracy: 0.6573\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 124s 2ms/step - loss: 1.0910 - accuracy: 0.6111 - val_loss: 0.9641 - val_accuracy: 0.6571\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.0668 - accuracy: 0.6205 - val_loss: 0.9245 - val_accuracy: 0.6713\n",
      "Epoch 9/10\n",
      " 4320/50000 [=>............................] - ETA: 1:38 - loss: 1.0530 - accuracy: 0.6206"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
    "model = MiniVGGNet.build(width=16, height=16, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=BS, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "preds = model.predict(testX)\n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1),\n",
    "\ttarget_names=labelNames))\n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
